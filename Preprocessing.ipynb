{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRO PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\M S\n",
      "[nltk_data]     I\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to C:\\Users\\M S\n",
      "[nltk_data]     I\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\M S\n",
      "[nltk_data]     I\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\M S\n",
      "[nltk_data]     I\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "100%|██████████| 500/500 [00:00<00:00, 9014.97it/s]\n",
      "100%|██████████| 500/500 [01:00<00:00,  8.30it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 329637.22it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 420861.33it/s]\n",
      "100%|██████████| 500/500 [00:04<00:00, 121.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessing selesai! Data tersimpan di DatasetShopee_Bersih.csv & DatasetShopee_Bersih.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import unicodedata\n",
    "import emoji\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords, wordnet, sentiwordnet as swn\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Load Data\n",
    "FILE_PATH = \"DatasetShopee.csv\"\n",
    "df = pd.read_csv(FILE_PATH)\n",
    "\n",
    "# Pilih rentang data\n",
    "START_ROW = 2\n",
    "END_ROW = 502\n",
    "df = df.iloc[START_ROW:END_ROW].dropna(subset=['comment'])\n",
    "\n",
    "# Fungsi normalisasi huruf berulang dengan pengecekan kamus\n",
    "kamus_kata_baku = {\"mantaaaap\": \"mantap\", \"gacoooor\": \"gacor\", \"bagusss\": \"bagus\"}\n",
    "\n",
    "# REPEAT X\n",
    "EXCEPTION_WORDS = {\"saya\", \"kakak\", \"kuku\", \"lulus\", \"aplikasi\", \"mantap\"}\n",
    "def normalize_repeated_chars(text):\n",
    "    words = text.split()\n",
    "    normalized_words = [\n",
    "        word if word in EXCEPTION_WORDS else re.sub(r'(.)\\1+', r'\\1', word)\n",
    "        for word in words\n",
    "    ]\n",
    "    return \" \".join(normalized_words)\n",
    "\n",
    "# Fungsi pembersihan teks\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    text = emoji.replace_emoji(text, replace=\"\")\n",
    "    text = re.sub(r'[@#][A-Za-z0-9_]+|https?:\\/\\/\\S+', ' ', text)\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = normalize_repeated_chars(text)  # ⬅ Fungsi baru diterapkan di sini\n",
    "    return text.lower()\n",
    "\n",
    "# Terapkan preprocessing\n",
    "tqdm.pandas()\n",
    "df['comment_clean'] = df['comment'].progress_apply(preprocess_text)\n",
    "\n",
    "# Stemming dengan Sastrawi\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "df['comment_clean'] = df['comment_clean'].progress_apply(lambda x: stemmer.stem(x))\n",
    "\n",
    "# Stopwords Removal dengan pengecualian kata negatif\n",
    "nltk_stopwords = set(stopwords.words('indonesian'))\n",
    "sastrawi_stopwords = set(StopWordRemoverFactory().get_stop_words())\n",
    "additional_stopwords = {\"sebuah\", \"pada\", \"pun\", \"bahkan\", \"oleh\", \"hanya\", \"tentang\", \"ke\", \"dari\", \"yang\", \"ini\", \"itu\", \"dengan\", \"seperti\"}\n",
    "\n",
    "# Kata negatif yang tidak boleh dihapus\n",
    "negative_words = {\"tidak\", \"bukan\", \"belum\", \"jangan\", \"tanpa\"}\n",
    "\n",
    "# Gabungkan stopwords tetapi tetap mempertahankan kata negatif\n",
    "all_stopwords = (nltk_stopwords | sastrawi_stopwords | additional_stopwords) - negative_words\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in text.split() if word not in all_stopwords])\n",
    "\n",
    "df['comment_clean'] = df['comment_clean'].progress_apply(lambda x: remove_stopwords(x) if x.strip() else \"tidak ada komentar\")\n",
    "\n",
    "# Sentimen Lexicon-Based dengan penanganan kata negatif\n",
    "sentiment_lexicon = {\n",
    "    'bagus': 1, 'baik': 1, 'puas': 1, 'senang': 1, 'suka': 1,\n",
    "    'buruk': -1, 'jelek': -1, 'kecewa': -1, 'tidak puas': -1, 'gacor': 1, 'enak': 1,\n",
    "    'mantap': 1, 'luar biasa': 1, 'mengecewakan': -1, 'parah': -1, 'sampah': -1, 'terbaik': 1\n",
    "}\n",
    "\n",
    "def classify_sentiment(text):\n",
    "    if text == \"tidak ada komentar\":\n",
    "        return \"Neutral\"\n",
    "    \n",
    "    words = text.split()\n",
    "    score = 0\n",
    "    prev_word = \"\"\n",
    "\n",
    "    for word in words:\n",
    "        if word in negative_words:\n",
    "            prev_word = word\n",
    "            continue\n",
    "        sentiment = sentiment_lexicon.get(word, 0)\n",
    "        if prev_word and sentiment != 0:\n",
    "            sentiment *= -1  # Balik sentimen jika ada kata negatif sebelumnya\n",
    "            prev_word = \"\"\n",
    "        score += sentiment\n",
    "\n",
    "    return \"Positive\" if score > 0 else \"Negative\" if score < 0 else \"Neutral\"\n",
    "\n",
    "df['sentimen_lexicon'] = df['comment_clean'].progress_apply(classify_sentiment)\n",
    "\n",
    "# Sentimen dengan SentiWordNet\n",
    "def translate_to_english(indonesian_word):\n",
    "    synsets = wordnet.synsets(indonesian_word, lang='ind')\n",
    "    return synsets[0].lemmas()[0].name() if synsets else indonesian_word\n",
    "\n",
    "def get_sentiwordnet_score(word):\n",
    "    english_word = translate_to_english(word)\n",
    "    if english_word == word:\n",
    "        return 0\n",
    "    \n",
    "    synsets = wordnet.synsets(english_word)\n",
    "    if not synsets:\n",
    "        return 0\n",
    "\n",
    "    synset = synsets[0]\n",
    "    senti_synset = swn.senti_synset(synset.name())\n",
    "    return senti_synset.pos_score() - senti_synset.neg_score()\n",
    "\n",
    "def classify_sentiment_swn(text):\n",
    "    words = text.split()\n",
    "    score = sum(get_sentiwordnet_score(word) for word in words)\n",
    "    return \"Positive\" if score > 0 else \"Negative\" if score < 0 else \"Neutral\"\n",
    "\n",
    "df['sentimen_swn'] = df['comment_clean'].progress_apply(classify_sentiment_swn)\n",
    "\n",
    "# Simpan hasil\n",
    "OUTPUT_CSV = \"DatasetShopee_Bersih.csv\"\n",
    "OUTPUT_PARQUET = \"DatasetShopee_Bersih.parquet\"\n",
    "\n",
    "df[['username', 'rating', 'comment_clean', 'sentimen_lexicon', 'sentimen_swn']].to_csv(OUTPUT_CSV, index=False)\n",
    "df[['username', 'rating', 'comment_clean', 'sentimen_lexicon', 'sentimen_swn']].to_parquet(OUTPUT_PARQUET, index=False)\n",
    "\n",
    "print(f\"✅ Preprocessing selesai! Data tersimpan di {OUTPUT_CSV} & {OUTPUT_PARQUET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIGDATA PREPROCESS OPTIMAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\M S\n",
      "[nltk_data]     I\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"MODIN_ENGINE\"] = \"dask\"\n",
    "\n",
    "import modin.pandas as pd\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import emoji\n",
    "import nltk\n",
    "import symspellpy\n",
    "from tqdm import tqdm\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Load Data\n",
    "FILE_PATH = \"DatasetShopee.csv\"\n",
    "df = pd.read_csv(FILE_PATH)\n",
    "\n",
    "# Pilih rentang data\n",
    "START_ROW, END_ROW = 2, 502\n",
    "df = df.iloc[START_ROW:END_ROW].dropna(subset=['comment']).copy()\n",
    "\n",
    "# Inisialisasi SymSpell untuk koreksi ejaan\n",
    "sym_spell = symspellpy.SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "sym_spell.load_dictionary(\"indonesian_words.txt\", term_index=0, count_index=1)  # Pastikan file kamus tersedia\n",
    "\n",
    "# Kamus kata baku dan pengecualian\n",
    "kamus_kata_baku = {\"mantaaaap\": \"mantap\", \"gacoooor\": \"gacor\", \"bagusss\": \"bagus\"}\n",
    "EXCEPTION_WORDS = {\"saya\", \"kakak\", \"kuku\", \"lulus\", \"aplikasi\", \"mantap\"}\n",
    "\n",
    "def normalize_repeated_chars(text):\n",
    "    words = text.split()\n",
    "    normalized_words = []\n",
    "    for word in words:\n",
    "        if word in EXCEPTION_WORDS:\n",
    "            normalized_words.append(word)\n",
    "        else:\n",
    "            cleaned_word = re.sub(r'(.)\\1{2,}', r'\\1', word)  # Hapus huruf berulang\n",
    "            lookup_result = sym_spell.lookup(cleaned_word, symspellpy.Verbosity.CLOSEST, max_edit_distance=2)\n",
    "            corrected_word = kamus_kata_baku.get(cleaned_word, lookup_result[0].term if lookup_result else cleaned_word)\n",
    "            normalized_words.append(corrected_word)\n",
    "    return \" \".join(normalized_words)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    text = emoji.replace_emoji(text, replace=\"\")\n",
    "    text = re.sub(r'[@#][A-Za-z0-9_]+|https?:\\/\\/\\S+', ' ', text)\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return normalize_repeated_chars(text).lower()\n",
    "\n",
    "tqdm.pandas()\n",
    "df['comment_clean'] = df['comment'].progress_apply(preprocess_text)\n",
    "\n",
    "# Stemming\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "df['comment_clean'] = df['comment_clean'].progress_apply(stemmer.stem)\n",
    "\n",
    "# Stopword Removal\n",
    "nltk_stopwords = set(nltk.corpus.stopwords.words('indonesian'))\n",
    "sastrawi_stopwords = set(StopWordRemoverFactory().get_stop_words())\n",
    "additional_stopwords = {\"sebuah\", \"pada\", \"pun\", \"bahkan\", \"oleh\", \"hanya\", \"tentang\", \"ke\", \"dari\", \"yang\", \"ini\", \"itu\", \"dengan\", \"seperti\"}\n",
    "negative_words = {\"tidak\", \"bukan\", \"belum\", \"jangan\", \"tanpa\"}\n",
    "all_stopwords = (nltk_stopwords | sastrawi_stopwords | additional_stopwords) - negative_words\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in text.split() if word not in all_stopwords])\n",
    "\n",
    "df['comment_clean'] = df['comment_clean'].progress_apply(lambda x: remove_stopwords(x) if x.strip() else \"tidak ada komentar\")\n",
    "\n",
    "# Sentimen dengan IndoBERT (Optimasi Batch Processing)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"indobenchmark/indobert-lite-base-p1\", device=0 if device == \"cuda\" else -1)\n",
    "\n",
    "def batch_sentiment(texts, batch_size=64):\n",
    "    results = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        with torch.no_grad():\n",
    "            batch_results = sentiment_pipeline(batch)\n",
    "        results.extend([res['label'] for res in batch_results])\n",
    "    return results\n",
    "\n",
    "df['sentimen_bert'] = batch_sentiment(df['comment_clean'].tolist())\n",
    "\n",
    "# Simpan hasil dalam format optimal\n",
    "OUTPUT_PARQUET = \"DatasetShopee_Bersih.parquet\"\n",
    "OUTPUT_FEATHER = \"DatasetShopee_Bersih.feather\"\n",
    "df[['username', 'rating', 'comment_clean', 'sentimen_bert']].to_parquet(OUTPUT_PARQUET, compression=\"snappy\", index=False)\n",
    "df[['username', 'rating', 'comment_clean', 'sentimen_bert']].to_feather(OUTPUT_FEATHER)\n",
    "\n",
    "print(f\"✅ Preprocessing selesai! Data tersimpan di {OUTPUT_PARQUET} & {OUTPUT_FEATHER}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
